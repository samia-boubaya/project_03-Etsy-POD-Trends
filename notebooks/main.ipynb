{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0661caf3",
   "metadata": {},
   "source": [
    "==================================================================================================================================\n",
    "# <div align=\"center\">PROJECT 03: Etsy Print-On-Demand Trends</div>\n",
    "=================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5dbfaa",
   "metadata": {},
   "source": [
    "### üìù BUSINESS IDEA\n",
    "\n",
    "**Print-On-Demand (POD) Business** ‚Äì What the project is about\n",
    "\n",
    "### ‚ÅâÔ∏è PROBLEM\n",
    "\n",
    "No API exists to access the market data needed, requiring web scraping to gather insights ‚Äì The challenge we‚Äôre addressing\n",
    "\n",
    "### üî∞ SOLUTION FRAMEWORK\n",
    "\n",
    "Web scrape etsy for a specific POD product\n",
    "\n",
    "Collect the data necessary to clean & analyze\n",
    "\n",
    "\n",
    "| **Development**                                                                                                                                             | **Presentation**                 |\n",
    "| ----------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------- |\n",
    "| **Business Idea** ‚Üí **Problem Definition** ‚Üí **Data Research & Visualization** ‚Üí **Insights** ‚Üí **Interpretation** ‚Üí **Implications** ‚Üí **Business Impact** | **Limitations & Considerations** |\n",
    "\n",
    "### üìå SECTION OVERVIEW\n",
    "\n",
    "* **Project / Business Idea:** What the project is about\n",
    "* **Problem:** The challenge we‚Äôre addressing\n",
    "* **Solution / Approach:** How we solve it\n",
    "* **Research & Plots:** How we analyzed data visually\n",
    "* **Insights:** What we discovered\n",
    "* **Interpretation:** Why it matters\n",
    "* **Implications:** What actions the business can take\n",
    "* **Business Impact:** Expected results for the business\n",
    "* **Limitations:** What constraints or gaps exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d8ffe",
   "metadata": {},
   "source": [
    "==================================================================================================================================\n",
    "# <div align=\"center\">WEB SCRAPING</div>\n",
    "=================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cbcc66",
   "metadata": {},
   "source": [
    "```Etsy``` is a dynamic website, so scraping it requires careful handling.\n",
    "\n",
    "Since ```Etsy``` uses ```JavaScript``` to load some content,\n",
    "\n",
    "```requests``` +  ``BeautifulSoup`` might work for static parts (like search results), \n",
    "\n",
    "but for dynamic content, ``Selenium`` is more reliable. \n",
    "\n",
    "I will be using ``requests`` + ``BeautifulSoup`` for ```product listings``` **(title, price, link)**\n",
    "\n",
    "Important Note: Etsy uses dynamic loading + anti-bot protections.\n",
    "\n",
    "Using code with standard HTML scraping can work as long as Etsy doesn‚Äôt block the request.\n",
    "\n",
    "If blocked, using headers, rotating proxies, or the Etsy API will be required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e65738d",
   "metadata": {},
   "source": [
    "=================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cc45c5",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e239d809",
   "metadata": {},
   "source": [
    "### Avoiding getting blocked\n",
    "| Version                                   | Best For          | Pros                                           | Cons                          |\n",
    "| ----------------------------------------- | ----------------- | ---------------------------------------------- | ----------------------------- |\n",
    "| **Requests + BeautifulSoup + Pagination** | Simple scraping   | Fast, clean                                    | Etsy may block request        |\n",
    "| **Selenium + BeautifulSoup + Pagination** | Reliable scraping | Bypasses bot protection, loads dynamic content | Slower, requires ChromeDriver |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6873625",
   "metadata": {},
   "source": [
    "#### üß∞ **Install for web scraping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd5545aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\sboub\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\sboub\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\sboub\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: selenium in c:\\users\\sboub\\anaconda3\\lib\\site-packages (4.38.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\sboub\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.10.5 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (2025.10.5)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (24.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.2)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio<1.0,>=0.31.0->selenium) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in c:\\users\\sboub\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "# install requests & beautifulsoup\n",
    "!pip install requests beautifulsoup4 fake-useragent pandas\n",
    "\n",
    "# install selenium\n",
    "!pip install selenium pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bb6fca",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d8c4a8",
   "metadata": {},
   "source": [
    "### üìå Pagination + BeautifulSoup Version\n",
    "| Version                                   | Best For          | Pros                                           | Cons                          |\n",
    "| ----------------------------------------- | ----------------- | ---------------------------------------------- | ----------------------------- |\n",
    "| **Requests + BeautifulSoup + Pagination** | Simple scraping   | Fast, clean                                    | Etsy may block request        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef6d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "\n",
    "def scrape_products(pages=5, max_items=10):\n",
    "    base_url = \"https://www.etsy.com/search?q=tote+bag&page=\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        url = base_url + str(page)\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        products = soup.find_all(\"li\", class_=\"wt-list-unstyled\")\n",
    "\n",
    "        for item in products:\n",
    "            if len(data) >= max_items:\n",
    "                return pd.DataFrame(data)\n",
    "\n",
    "            # URL\n",
    "            link = item.find(\"a\", href=True)\n",
    "            if not link:\n",
    "                continue\n",
    "            product_url = \"https://www.etsy.com\" + link[\"href\"]\n",
    "\n",
    "            # Title\n",
    "            title_tag = item.find(\"h3\")\n",
    "            title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "            # Price\n",
    "            price_tag = item.find(\"span\", class_=\"currency-value\")\n",
    "            price = None\n",
    "            if price_tag:\n",
    "                try:\n",
    "                    price = float(price_tag.text.replace(\",\", \".\"))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # Rating\n",
    "            rating_tag = item.find(\"span\", class_=\"wt-screen-reader-only\")\n",
    "            rating = None\n",
    "            if rating_tag:\n",
    "                match_rating = re.search(r\"([\\d.]+) out of 5\", rating_tag.text)\n",
    "                if match_rating:\n",
    "                    rating = float(match_rating.group(1))\n",
    "\n",
    "            # Reviews\n",
    "            reviews_tag = item.find(\"span\", class_=\"wt-text-body-01\")\n",
    "            reviews = None\n",
    "            if reviews_tag:\n",
    "                match_reviews = re.search(r\"\\((\\d+)\\)\", reviews_tag.text)\n",
    "                if match_reviews:\n",
    "                    reviews = int(match_reviews.group(1))\n",
    "\n",
    "            # Delivery\n",
    "            delivery = None\n",
    "            delivery_tag = item.find(string=re.compile(\"delivery\", re.I))\n",
    "            if delivery_tag:\n",
    "                txt = delivery_tag.lower()\n",
    "                if \"free\" in txt:\n",
    "                    delivery = 0\n",
    "                else:\n",
    "                    match_del = re.search(r\"‚Ç¨\\s?([\\d.,]+)\", delivery_tag)\n",
    "                    if match_del:\n",
    "                        delivery = float(match_del.group(1).replace(\",\", \".\"))\n",
    "\n",
    "            data.append({\n",
    "                \"URL\": product_url,\n",
    "                \"Title\": title,\n",
    "                \"Price\": price,\n",
    "                \"Rating\": rating,\n",
    "                \"Reviews\": reviews,\n",
    "                \"Delivery\": delivery\n",
    "            })\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Example: save CSV\n",
    "if __name__ == \"__main__\":\n",
    "    df = scrape_products()\n",
    "    df.to_csv(\"../data/interim/0_interim_price.csv\", index=False)\n",
    "    print(\"STEP 1 : 'Price' INTERIM and CSV saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861abf5a",
   "metadata": {},
   "source": [
    "### üìå Selenium-Based Version (ChromeDriver)\n",
    "\n",
    "| Version                                   | Best For          | Pros                                           | Cons                          |\n",
    "| ----------------------------------------- | ----------------- | ---------------------------------------------- | ----------------------------- |\n",
    "| **Selenium + BeautifulSoup + Pagination** | Reliable scraping | Bypasses bot protection, loads dynamic content | Slower, requires ChromeDriver |\n",
    "\n",
    "Link to ChromeDriver: https://googlechromelabs.github.io/chrome-for-testing/#stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "def scrape_products_selenium(max_items=10):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  \n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    data = []\n",
    "    page = 1\n",
    "\n",
    "    while len(data) < max_items:\n",
    "        url = f\"https://www.etsy.com/search?q=tote+bag&page={page}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(4)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        products = soup.find_all(\"li\", class_=\"wt-list-unstyled\")\n",
    "\n",
    "        for item in products:\n",
    "            if len(data) >= max_items:\n",
    "                break\n",
    "\n",
    "            # URL\n",
    "            link = item.find(\"a\", href=True)\n",
    "            if not link:\n",
    "                continue\n",
    "            product_url = \"https://www.etsy.com\" + link[\"href\"]\n",
    "\n",
    "            # Title\n",
    "            title_tag = item.find(\"h3\")\n",
    "            title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "            # Price\n",
    "            price_tag = item.find(\"span\", class_=\"currency-value\")\n",
    "            price = None\n",
    "            if price_tag:\n",
    "                try:\n",
    "                    price = float(price_tag.text.replace(\",\", \".\"))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # Rating\n",
    "            rating_tag = item.find(\"span\", class_=\"wt-screen-reader-only\")\n",
    "            rating = None\n",
    "            if rating_tag:\n",
    "                match_rating = re.search(r\"([\\d.]+) out of 5\", rating_tag.text)\n",
    "                if match_rating:\n",
    "                    rating = float(match_rating.group(1))\n",
    "\n",
    "            # Reviews\n",
    "            reviews_tag = item.find(\"span\", class_=\"wt-text-body-01\")\n",
    "            reviews = None\n",
    "            if reviews_tag:\n",
    "                match_reviews = re.search(r\"\\((\\d+)\\)\", reviews_tag.text)\n",
    "                if match_reviews:\n",
    "                    reviews = int(match_reviews.group(1))\n",
    "\n",
    "            # Delivery\n",
    "            delivery = None\n",
    "            delivery_tag = item.find(string=re.compile(\"delivery\", re.I))\n",
    "            if delivery_tag:\n",
    "                txt = delivery_tag.lower()\n",
    "                if \"free\" in txt:\n",
    "                    delivery = 0\n",
    "                else:\n",
    "                    match_del = re.search(r\"‚Ç¨\\s?([\\d.,]+)\", delivery_tag)\n",
    "                    if match_del:\n",
    "                        delivery = float(match_del.group(1).replace(\",\", \".\"))\n",
    "\n",
    "            data.append({\n",
    "                \"URL\": product_url,\n",
    "                \"Title\": title,\n",
    "                \"Price\": price,\n",
    "                \"Rating\": rating,\n",
    "                \"Reviews\": reviews,\n",
    "                \"Delivery\": delivery\n",
    "            })\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(2)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Save CSV\n",
    "if __name__ == \"__main__\":\n",
    "    df = scrape_products_selenium()\n",
    "    df.to_csv(\"../data/interim/1_interim_price.csv\", index=False)\n",
    "    print(\"STEP 1 : 'Price' INTERIM and CSV saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c884e4f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c07f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Etsy Tote Bag Scraper (Selenium + BeautifulSoup) with:\n",
    "- Pagination\n",
    "- Proxy rotation\n",
    "- Random user-agents\n",
    "- Class-based design\n",
    "- Adjustable product limit\n",
    "Saves final cleaned dataframe to ../data/clean/clean_data.csv\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import WebDriverException, TimeoutException\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EtsyToteScraper:\n",
    "    user_agents: List[str] = field(default_factory=lambda: [\n",
    "        # A short sample; replace/extend with more UAs for real rotations\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/120 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_0) AppleWebKit/605.1.15 \"\n",
    "        \"(KHTML, like Gecko) Version/16.0 Safari/605.1.15\",\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/117 Safari/537.36\"\n",
    "    ])\n",
    "    proxies: List[str] = field(default_factory=list)  # e.g. [\"http://ip:port\", \"http://user:pass@ip:port\"]\n",
    "    chromedriver_path: Optional[str] = None  # if None assumes chromedriver is on PATH\n",
    "    headless: bool = True\n",
    "    page_load_wait: float = 3.5  # seconds to wait after loading a page\n",
    "    max_restarts_for_errors: int = 2\n",
    "\n",
    "    def _make_driver(self, proxy: Optional[str], user_agent: str):\n",
    "        \"\"\"Create a Selenium Chrome WebDriver with given proxy & user agent.\"\"\"\n",
    "        options = Options()\n",
    "        if self.headless:\n",
    "            options.add_argument(\"--headless=new\")  # use new headless mode\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--window-size=1400,1000\")\n",
    "        options.add_argument(f\"--user-agent={user_agent}\")\n",
    "\n",
    "        if proxy:\n",
    "            # Set proxy; Chrome expects --proxy-server argument\n",
    "            options.add_argument(f'--proxy-server={proxy}')\n",
    "\n",
    "        # Optional: reduce webdriver fingerprint\n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "        try:\n",
    "            if self.chromedriver_path:\n",
    "                driver = webdriver.Chrome(executable_path=self.chromedriver_path, options=options)  # type: ignore\n",
    "            else:\n",
    "                driver = webdriver.Chrome(options=options)\n",
    "        except TypeError:\n",
    "            # Some selenium versions use service object; fallback to default constructor\n",
    "            driver = webdriver.Chrome(options=options)  # type: ignore\n",
    "        return driver\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_price(price_text: str) -> Optional[float]:\n",
    "        if not price_text:\n",
    "            return None\n",
    "        # Normalize and extract first price-looking token (handles \"‚Ç¨12.50\" and \"12,50 ‚Ç¨\")\n",
    "        price_text = price_text.strip()\n",
    "        # Keep euro symbol and digits, commas, dots\n",
    "        m = re.search(r\"‚Ç¨\\s*([\\d\\.,]+)|([\\d\\.,]+)\\s*‚Ç¨\", price_text)\n",
    "        if m:\n",
    "            num = m.group(1) or m.group(2)\n",
    "        else:\n",
    "            # fallback: find any number-like substring\n",
    "            m2 = re.search(r\"([\\d]{1,3}(?:[.,]\\d{1,3})+|\\d+)\", price_text)\n",
    "            if not m2:\n",
    "                return None\n",
    "            num = m2.group(1)\n",
    "        # convert to float, handling comma as decimal if needed\n",
    "        num = num.replace(\".\", \"\").replace(\",\", \".\") if num.count(\",\") == 1 and num.count(\".\") == 0 else num.replace(\",\", \"\")\n",
    "        try:\n",
    "            return float(num)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_rating(text: str) -> Optional[float]:\n",
    "        if not text:\n",
    "            return None\n",
    "        m = re.search(r\"([0-5](?:\\.[0-9])?)\\s*out of\\s*5\", text, re.I)\n",
    "        if m:\n",
    "            try:\n",
    "                return float(m.group(1))\n",
    "            except:\n",
    "                return None\n",
    "        # sometimes rating appears as \"4.8\" alone\n",
    "        m2 = re.search(r\"\\b([0-5]\\.\\d)\\b\", text)\n",
    "        if m2:\n",
    "            try:\n",
    "                return float(m2.group(1))\n",
    "            except:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_reviews(text: str) -> Optional[int]:\n",
    "        if not text:\n",
    "            return None\n",
    "        # look for parentheses e.g. \"(123)\" or \"123 reviews\"\n",
    "        m = re.search(r\"\\((\\d{1,6})\\)\", text.replace(\"\\xa0\", \" \"))\n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "        m2 = re.search(r\"(\\d{1,6})\\s+review\", text, re.I)\n",
    "        if m2:\n",
    "            return int(m2.group(1))\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _clean_text(elem):\n",
    "        return elem.get_text(\" \", strip=True) if elem else \"\"\n",
    "\n",
    "    def scrape(self, max_items: int = 10, max_pages: int = 20, start_page: int = 1) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrape Etsy tote bag products.\n",
    "\n",
    "        Parameters:\n",
    "        - max_items: total number of product rows to collect (default 10)\n",
    "        - max_pages: maximum pages to visit (safety cap)\n",
    "        - start_page: which search page to start from (1-based)\n",
    "        \"\"\"\n",
    "        data_rows = []\n",
    "        page = start_page\n",
    "        attempts = 0\n",
    "\n",
    "        # We'll periodically rotate proxy & UA by restarting the driver\n",
    "        while len(data_rows) < max_items and page < start_page + max_pages:\n",
    "            # choose random UA & proxy\n",
    "            ua = random.choice(self.user_agents)\n",
    "            proxy = random.choice(self.proxies) if self.proxies else None\n",
    "\n",
    "            restarts = 0\n",
    "            while restarts <= self.max_restarts_for_errors:\n",
    "                driver = None\n",
    "                try:\n",
    "                    driver = self._make_driver(proxy, ua)\n",
    "                    search_url = f\"https://www.etsy.com/search?q=tote+bag&page={page}\"\n",
    "                    print(f\"[INFO] Loading page {page} (collected {len(data_rows)}/{max_items}) ‚Äî UA chosen, proxy={proxy is not None}\")\n",
    "                    driver.get(search_url)\n",
    "                    time.sleep(self.page_load_wait + random.uniform(0.5, 2.0))  # allow JS to load\n",
    "\n",
    "                    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "                    # Etsy product tiles: use `li` elements with data-search-result or a result class\n",
    "                    product_items = soup.find_all(\"li\", attrs={\"data-search-result\": True})\n",
    "                    if not product_items:\n",
    "                        # fallback heuristics (sometimes different structure)\n",
    "                        product_items = soup.find_all(\"div\", class_=re.compile(r\"v2-listing-card|search-result|listing-link|wt-grid-item\"), limit=60)\n",
    "\n",
    "                    if not product_items:\n",
    "                        print(\"[WARN] No product items found on the page. The markup might have changed.\")\n",
    "                        break\n",
    "\n",
    "                    for item in product_items:\n",
    "                        if len(data_rows) >= max_items:\n",
    "                            break\n",
    "\n",
    "                        # URL\n",
    "                        link_tag = item.find(\"a\", href=True)\n",
    "                        if not link_tag:\n",
    "                            continue\n",
    "                        product_url = link_tag[\"href\"].split(\"?\")[0]  # remove query params\n",
    "\n",
    "                        # Title\n",
    "                        title = None\n",
    "                        title_tag = item.find(\"h3\")\n",
    "                        if title_tag:\n",
    "                            title = title_tag.get_text(\" \", strip=True)\n",
    "                        else:\n",
    "                            # alternative\n",
    "                            title_tag2 = item.find(\"h2\") or item.find(\"p\", class_=re.compile(\"title|text\"))\n",
    "                            title = title_tag2.get_text(\" \", strip=True) if title_tag2 else \"\"\n",
    "\n",
    "                        # Price - try several selectors\n",
    "                        price = None\n",
    "                        # Etsy often uses <span class=\"currency-value\">12.00</span>\n",
    "                        price_span = item.find(\"span\", class_=re.compile(r\"currency-value|listing-price\"))\n",
    "                        if price_span:\n",
    "                            price = self._parse_price(price_span.get_text(\" \", strip=True))\n",
    "                        else:\n",
    "                            # try to extract from any text snippet in this tile\n",
    "                            combined_text = self._clean_text(item)\n",
    "                            # find euro price in combined text\n",
    "                            price = self._parse_price(combined_text)\n",
    "\n",
    "                        # Rating - try screen-reader text or aria labels\n",
    "                        rating = None\n",
    "                        rating_span = item.find(\"span\", class_=re.compile(r\"screen-reader-only|text-body-01|sr-only\"), string=re.compile(r\"out of 5\", re.I))\n",
    "                        if rating_span:\n",
    "                            rating = self._extract_rating(rating_span.get_text(\" \", strip=True))\n",
    "                        else:\n",
    "                            # try aria-label on an element\n",
    "                            rating_aria = item.find(attrs={\"aria-label\": re.compile(r\"out of 5\", re.I)})\n",
    "                            if rating_aria:\n",
    "                                rating = self._extract_rating(rating_aria[\"aria-label\"])\n",
    "\n",
    "                        # Reviews - look for parentheses or \"reviews\" nearby\n",
    "                        reviews = None\n",
    "                        # check for small count element\n",
    "                        reviews_candidates = item.find_all(text=re.compile(r\"\\(\\d+\\)|\\d+\\s+review\", re.I))\n",
    "                        if reviews_candidates:\n",
    "                            for cand in reviews_candidates:\n",
    "                                r = self._extract_reviews(cand)\n",
    "                                if r:\n",
    "                                    reviews = r\n",
    "                                    break\n",
    "                        if reviews is None:\n",
    "                            # fallback to searching whole tile text\n",
    "                            reviews = self._extract_reviews(self._clean_text(item))\n",
    "\n",
    "                        # Delivery - detect Free shipping or shipping cost\n",
    "                        delivery = None\n",
    "                        # Common pattern: \"Free shipping\", \"Free standard shipping\", or \"Shipping: ‚Ç¨3.00\"\n",
    "                        shipping_texts = item.find_all(text=re.compile(r\"free shipping|shipping|delivery\", re.I))\n",
    "                        if shipping_texts:\n",
    "                            for st in shipping_texts:\n",
    "                                st_lower = st.strip().lower()\n",
    "                                if \"free\" in st_lower:\n",
    "                                    delivery = 0\n",
    "                                    break\n",
    "                                # try to parse euro amount\n",
    "                                parsed = self._parse_price(st)\n",
    "                                if parsed is not None:\n",
    "                                    delivery = parsed\n",
    "                                    break\n",
    "                        if delivery is None:\n",
    "                            # look at the product page (optional expensive step) - skip to save time\n",
    "\n",
    "                            # default to None if unknown\n",
    "                            delivery = None\n",
    "\n",
    "                        data_rows.append({\n",
    "                            \"URL\": product_url,\n",
    "                            \"Title\": title,\n",
    "                            \"Price\": price,\n",
    "                            \"Rating\": rating,\n",
    "                            \"Reviews\": reviews,\n",
    "                            \"Delivery\": delivery\n",
    "                        })\n",
    "\n",
    "                    # Page completed\n",
    "                    driver.quit()\n",
    "                    break  # break restart loop on success\n",
    "\n",
    "                except (WebDriverException, TimeoutException) as e:\n",
    "                    print(f\"[ERROR] WebDriver error: {e} ‚Äî restarting driver (attempt {restarts+1})\")\n",
    "                    if driver:\n",
    "                        try:\n",
    "                            driver.quit()\n",
    "                        except:\n",
    "                            pass\n",
    "                    restarts += 1\n",
    "                    time.sleep(1 + random.random() * 2)\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] Unexpected error parsing page {page}: {e}\")\n",
    "                    if driver:\n",
    "                        try:\n",
    "                            driver.quit()\n",
    "                        except:\n",
    "                            pass\n",
    "                    restarts += 1\n",
    "                    time.sleep(1 + random.random() * 2)\n",
    "\n",
    "            page += 1\n",
    "            attempts += 1\n",
    "            # polite pause between page loads and to reduce detection risk\n",
    "            time.sleep(1.0 + random.uniform(0.8, 2.2))\n",
    "\n",
    "        # Build DataFrame with exactly up to max_items rows (trim if needed)\n",
    "        df = pd.DataFrame(data_rows)[:max_items]\n",
    "\n",
    "        # Normalize columns: ensure numeric types where possible\n",
    "        if not df.empty:\n",
    "            df['Price'] = pd.to_numeric(df['Price'], errors='coerce')\n",
    "            df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
    "            df['Reviews'] = pd.to_numeric(df['Reviews'], errors='coerce').astype('Int64')\n",
    "            # Delivery: treat None as NaN; where 0 -> free shipping\n",
    "            df['Delivery'] = pd.to_numeric(df['Delivery'], errors='coerce')\n",
    "\n",
    "        # Save CSV as requested\n",
    "        out_path = os.path.join(\"..\", \"data\", \"clean\", \"clean_data.csv\")\n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        df.to_csv(out_path, index=False)\n",
    "        print(\"STEP 1 : 'Price' CLEAN and CSV saved successfully!\")\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # === Example usage ===\n",
    "    # Provide your proxies and optionally a larger user-agent list\n",
    "    proxies = [\n",
    "        # \"http://user:pass@12.34.56.78:1234\",\n",
    "        # \"http://12.34.56.79:8080\",\n",
    "    ]\n",
    "\n",
    "    user_agents = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/120 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_0) AppleWebKit/605.1.15 \"\n",
    "        \"(KHTML, like Gecko) Version/16.0 Safari/605.1.15\",\n",
    "        # add more UAs here...\n",
    "    ]\n",
    "\n",
    "    scraper = EtsyToteScraper(\n",
    "        user_agents=user_agents,\n",
    "        proxies=proxies,\n",
    "        chromedriver_path=None,  # or set path like \"/usr/local/bin/chromedriver\"\n",
    "        headless=True,\n",
    "        page_load_wait=3.5\n",
    "    )\n",
    "\n",
    "    print(\"[START] Scraping up to 10 tote bag products (Selenium + rotating UA/proxy)...\")\n",
    "    df = scraper.scrape(max_items=10, max_pages=30, start_page=1)\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc33f6",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f758efb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Chrome...\n",
      "Page title: Google\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "\n",
    "print(\"Launching Chrome...\")\n",
    "\n",
    "# launch browser\n",
    "driver = uc.Chrome()\n",
    "\n",
    "driver.get(\"https://www.google.com\")\n",
    "\n",
    "print(\"Page title:\", driver.title)\n",
    "\n",
    "time.sleep(5)\n",
    "driver.quit()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c4f6a9",
   "metadata": {},
   "source": [
    "### WEB SCRAPER INTERIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb4bec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 10 : TOTE BAGS and CSV saved successfully!\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "def scrape_products(limit=10):\n",
    "    \"\"\"\n",
    "    Scrape tote bag product data from Etsy using Selenium + BeautifulSoup.\n",
    "    Includes pagination & anti-bot avoidance.\n",
    "    Returns a pandas DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Launch undetected Chrome\n",
    "    driver = uc.Chrome()\n",
    "    driver.maximize_window()\n",
    "\n",
    "    # Etsy tote bags search\n",
    "    url = \"https://www.etsy.com/search?q=tote+bag\"\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    products = []\n",
    "\n",
    "    while len(products) < limit:\n",
    "        # Scroll to load products\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # All product cards\n",
    "        items = soup.select(\"li.wt-list-unstyled\")  # Etsy product item containers\n",
    "\n",
    "        for item in items:\n",
    "            if len(products) >= limit:\n",
    "                break\n",
    "\n",
    "            # URL\n",
    "            url_tag = item.select_one(\"a.listing-link\")\n",
    "            if not url_tag:\n",
    "                continue\n",
    "            product_url = url_tag.get(\"href\")\n",
    "\n",
    "            # Title\n",
    "            title_tag = item.select_one(\"h3\")\n",
    "            title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "            # Price\n",
    "            price_tag = item.select_one(\".currency-value\")\n",
    "            price = price_tag.get_text(strip=True) if price_tag else None\n",
    "\n",
    "            # Rating\n",
    "            rating_tag = item.select_one(\".wt-screen-reader-only\")\n",
    "            rating = None\n",
    "            if rating_tag:\n",
    "                # Example text: \"5 out of 5 stars\"\n",
    "                text = rating_tag.get_text(strip=True)\n",
    "                if \"out of 5 stars\" in text:\n",
    "                    rating = float(text.split(\" out\")[0])\n",
    "\n",
    "            # Reviews count\n",
    "            reviews_tag = item.select_one(\".wt-text-caption\")\n",
    "            reviews = None\n",
    "            if reviews_tag:\n",
    "                text = reviews_tag.get_text(strip=True)\n",
    "                # e.g. \"(123)\"\n",
    "                if text.startswith(\"(\") and text.endswith(\")\"):\n",
    "                    try:\n",
    "                        reviews = int(text.strip(\"()\"))\n",
    "                    except:\n",
    "                        reviews = None\n",
    "\n",
    "            # Delivery price (if available)\n",
    "            delivery_tag = item.select_one(\".wt-text-strikethrough, .wt-text-muted\")\n",
    "            delivery = None\n",
    "            if delivery_tag:\n",
    "                delivery_text = delivery_tag.get_text(strip=True)\n",
    "                # Normalize delivery cost\n",
    "                if \"Free delivery\" in delivery_text or \"FREE\" in delivery_text:\n",
    "                    delivery = 0\n",
    "                else:\n",
    "                    delivery = delivery_text\n",
    "\n",
    "            products.append({\n",
    "                \"URL\": product_url,\n",
    "                \"Title\": title,\n",
    "                \"Price\": price,\n",
    "                \"Rating\": rating,\n",
    "                \"Reviews\": reviews,\n",
    "                \"Delivery\": delivery\n",
    "            })\n",
    "\n",
    "        # Go to next page if needed\n",
    "        if len(products) < limit:\n",
    "            next_button = None\n",
    "            try:\n",
    "                next_button = driver.find_element(By.CSS_SELECTOR, \"a[aria-label='Next page']\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if next_button:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    driver.quit()\n",
    "    return pd.DataFrame(products)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# EXECUTION\n",
    "# -----------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    df = scrape_products(limit=10)\n",
    "\n",
    "    # SAVE CSV\n",
    "    df.to_csv(\"../data/clean/clean_tote_bags.csv\", index=False)\n",
    "    print(\"STEP 10 : TOTE BAGS and CSV saved successfully!\")\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20ae272",
   "metadata": {},
   "source": [
    "==================================================================================================================================\n",
    "# <div align=\"center\">DATA CLEANING & ANALYSIS</div>\n",
    "=================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f53801",
   "metadata": {},
   "source": [
    "#### üóÉÔ∏è **Raw data**\n",
    "\n",
    "- Web scraped data saved in a DataFrame then a CSV file and uploaded to google drive\n",
    "- The df_url has to be a downloadable link to the csv file from google drive\n",
    "- We load the csv to use for data cleaning and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b562b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "df_url = 'link to the dataFrame collected from scraping as a downloadable link from google drive'\n",
    "df = pd.read_csv(df_url)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c19bfc",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa517ddd",
   "metadata": {},
   "source": [
    "#### üóÉÔ∏è **Interim data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caaa5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save 'Price' INTERIM to CSV\n",
    "df.to_csv(\"../data/interim/1_interim_price.csv\", index=False)\n",
    "print(\"STEP 1 : 'Price' INRTERIM and CSV saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae28f7a9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60838b5b",
   "metadata": {},
   "source": [
    "#### üóÉÔ∏è **Clean data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72bd2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save 'Price' CLEAN to CSV\n",
    "df.to_csv(\"../data/clean/1_clean_price.csv\", index=False)\n",
    "print(\"STEP 1 : 'Price' CLEAN and CSV saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f01c223",
   "metadata": {},
   "source": [
    "==================================================================================================================================\n",
    "# <div align=\"center\">RESEARCH</div>\n",
    "=================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc370b1",
   "metadata": {},
   "source": [
    "### üåê **Which Are the Best-Selling POD Products on Etsy?**\n",
    "\n",
    "I‚Äôm researching print-on-demand products to sell on Etsy that only require **digital artwork and marketing**, while the POD provider handles **printing, packaging, and shipping**.\n",
    "\n",
    "\n",
    "### ‚≠ê Using Google Trends for POD Product Research\n",
    "üí° **Goal:** Identify which POD product category has been searched the most on Google over the past 5 years (2020‚Äì2025).\n",
    "\n",
    "Below is the list of product categories I‚Äôm comparing:\n",
    "\n",
    "1. ```Custom Apparel```\n",
    "    - T-shirts  \n",
    "    - Hoodies  \n",
    "    - Sweatshirts  \n",
    "    - Tank tops \n",
    "\n",
    "2. ```Mug```\n",
    "    - Ceramic mugs  \n",
    "    - Color-changing mugs  \n",
    "    - Espresso mugs  \n",
    "    - Travel mugs \n",
    "\n",
    "3. ```Tote Bag```\n",
    "    - Cotton totes  \n",
    "    - All-over print totes  \n",
    "\n",
    "4. ```Phone Case```\n",
    "    - iPhone / Samsung cases  \n",
    "    - Tough / Slim cases  \n",
    "\n",
    "5. ```Stickers```\n",
    "    - Die-cut stickers  \n",
    "    - Kiss-cut stickers  \n",
    "    - Sticker sheets \n",
    "\n",
    "6. ```Hats```\n",
    "    - Baseball caps  \n",
    "    - Trucker hats  \n",
    "    - Beanies  \n",
    "\n",
    "7. ```Pillows / Cushions```\n",
    "    - Pillow covers  \n",
    "    - Stuffed pillows  \n",
    "    - All-over print pillow designs  \n",
    "\n",
    "8. ```Blanket```\n",
    "    - Fleece blankets  \n",
    "    - Sherpa blankets  \n",
    "    - Woven blankets  \n",
    "\n",
    "9. ```Wall Art```\n",
    "    - Posters  \n",
    "    - Canvas prints  \n",
    "    - Framed posters  \n",
    "    - Metal prints  \n",
    "\n",
    "10. ```Doormat```\n",
    "    - Printed coir doormats  \n",
    "    - Rubber-backed doormats \n",
    "\n",
    "11. ```Drinkware```\n",
    "    - Stainless steel tumblers  \n",
    "    - Water bottles  \n",
    "    - Wine tumblers \n",
    "\n",
    "12. ```Calendar```\n",
    "    - Custom printed wall calendars  \n",
    "\n",
    "13. ```Yoga Mat```\n",
    "    - Printed yoga mats \n",
    "\n",
    "14. ```Bedding```\n",
    "    - Duvet covers  \n",
    "    - Pillowcases  \n",
    "    - All-over print bed sets\n",
    "\n",
    "15. ```Pet Accessories```\n",
    "    - Pet bandanas  \n",
    "    - Pet beds  \n",
    "    - Pet bowls  \n",
    "    - Pet blankets  \n",
    "\n",
    "16. ```Ornaments```\n",
    "    - Ceramic ornaments\n",
    "    - Wood ornaments\n",
    "    - Metal ornaments \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a59c7d6",
   "metadata": {},
   "source": [
    "------\n",
    "### üéØ Chosen POD product to research is : tote bags\n",
    "\n",
    "aria-label=\"4.9 star rating with 398 reviews\"\n",
    "\n",
    "etsy store selling print on demand products\n",
    "\n",
    "data needed\n",
    "- product title keywords to use to optimize sales / using title\n",
    "- product description keywords / \n",
    "- insight the niches based on most selling keywords\n",
    "- period when to sell / using reviews\n",
    "- price / most selling price tag and range\n",
    "- targeted audience ?\n",
    "- how to market it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4b6165",
   "metadata": {},
   "source": [
    "Chosen website for Data Scraping : Etsy\n",
    "\n",
    "data to extract : \n",
    "\n",
    "- product_title, for the keywords used in it to analyse the niche of this POD product\n",
    "\n",
    "- product_price, for figuring the best price to sell it at\n",
    "\n",
    "- product_listing_date, the date this product got created and added on etsy \n",
    "\n",
    "- product_rating, to know which niche in this POD product is selling the most \n",
    "- product_niche_rating\n",
    "\n",
    "- product_reviews_date, to compare nbr_review vs nbr_orders \n",
    "and to have a plot showing the rating of this product over time\n",
    "when did those sales happen the most and if it was recent or not\n",
    "two products can be sold with the same amount of orders but\n",
    "at different lengths of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34c47e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_category : t-shirt, mug, calendar,...\n",
    "# product_niche : comedy, drama, horror, halloween, cartoon, anime, ... \n",
    "# product_price :  in euros\n",
    "# product_listing_date: 00/00/0000 date created and added to etsy on product page\n",
    "# product_rating: 0.0/5 current rating of the product to compare\n",
    "# product_reviews_ratings: DataFrame with reviews ratings of each product from product page\n",
    "# product_reviews_dates: DataFrame with reviews dates of each product from product page\n",
    "# product_reviews_date: DataFrame with reviews descriptions of each product from product page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cf6a73",
   "metadata": {},
   "source": [
    "==================================================================================================================================\n",
    "# <div align=\"center\">PLOTS</div>\n",
    "=================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b182f1df",
   "metadata": {},
   "source": [
    "### üìä PLOT 01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "001cf4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2b068a",
   "metadata": {},
   "source": [
    "### üìä PLOT 02:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa276733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1870d0",
   "metadata": {},
   "source": [
    "### üìä PLOT 03:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb8c1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5536c3",
   "metadata": {},
   "source": [
    "### üìä PLOT 04:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d36191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07984f8",
   "metadata": {},
   "source": [
    "### üìä PLOT 05:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee723ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399abc24",
   "metadata": {},
   "source": [
    "==================================================================================================================================\n",
    "# <div align=\"center\">INSIGHTS</div>\n",
    "=================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6361e53a",
   "metadata": {},
   "source": [
    "### üß† INSIGHT 01:\n",
    "Text\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7a05e5",
   "metadata": {},
   "source": [
    "### üß† INSIGHT 02:\n",
    "Text\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cbb21b",
   "metadata": {},
   "source": [
    "### üß† INSIGHT 03:\n",
    "Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011deb7a",
   "metadata": {},
   "source": [
    "=================================================================================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
